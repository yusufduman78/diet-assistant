{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-16T20:19:08.089841Z",
     "start_time": "2025-09-16T20:19:07.991008Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Local modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.model.dataset import create_data_loaders\n",
    "from src.model.transforms import get_train_transforms,get_val_transforms\n",
    "\n",
    "# Training settings\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Get reproducible results by controlling random\n",
    "torch.manual_seed(42)\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(f'Selected device: {DEVICE}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T20:19:09.378990Z",
     "start_time": "2025-09-16T20:19:08.985670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data and DataLoader\n",
    "data_path = \"../../data/processed/01_final_dataset\"\n",
    "\n",
    "train_loader,val_loader,test_loader,weights = create_data_loaders(\n",
    "    data_path,\n",
    "    get_train_transforms(IMG_SIZE),\n",
    "    get_val_transforms(IMG_SIZE),\n",
    "    BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    class_weights=True\n",
    ")\n",
    "\n",
    "# Class names\n",
    "with open('../../data/processed/01_metadata/class_names.json','r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "    NUM_CLASSES = len(class_to_idx)\n",
    "\n",
    "print(f\"Total class number: {NUM_CLASSES}\")\n",
    "print(f\"In train set: {len(train_loader.dataset)} images\")\n",
    "print(f\"In val set: {len(val_loader.dataset)} images\")\n",
    "print(f\"In test set: {len(test_loader.dataset)} images\")"
   ],
   "id": "68162c94137ca4f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102951 images loaded from dataset (train).\n",
      "22036 images loaded from dataset (val).\n",
      "22129 images loaded from dataset (test).\n",
      "Total class number: 197\n",
      "In train set: 102951 images\n",
      "In val set: 22036 images\n",
      "In test set: 22129 images\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T20:19:10.862782Z",
     "start_time": "2025-09-16T20:19:10.792664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_model(num_classes, model_name='mobilenet_v2', fine_tune=False):\n",
    "    \"\"\"\n",
    "    Download pretrained model and update last layer or fine-tuning\n",
    "    Args:\n",
    "    num_classes (int): Class number on dataset\n",
    "    model_name (str): name of the selected model\n",
    "    fine_tune (bool): Determines fine-tuning or last layer updating\n",
    "\n",
    "    Returns:\n",
    "    torch.nn.Module: Customized model object\n",
    "    \"\"\"\n",
    "    if model_name == 'mobilenet_v2':\n",
    "        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        # If fine_tune is false, freeze all layers except the last one\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = fine_tune\n",
    "        num_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_features, num_classes)\n",
    "        # The last layer should always be trainable\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif model_name == 'mobilenet_v3':\n",
    "        model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = fine_tune\n",
    "        num_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_features, num_classes)\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif model_name == 'resnet18':\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = fine_tune\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, num_classes)\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        print(f\"Error: '{model_name}' does not support. Please select 'mobilenet_v2', 'mobilenet_v3' or 'resnet18'.\")\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "# Start with fine_tune=True to ensure the entire network is trainable.\n",
    "model_name = 'mobilenet_v3'\n",
    "model = create_model(NUM_CLASSES, model_name, fine_tune=True)\n",
    "model.to(DEVICE)"
   ],
   "id": "530e8c5c98713435",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=197, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T20:19:11.928894Z",
     "start_time": "2025-09-16T20:19:11.922980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Kullanılan GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU bulunamadı. Kod CPU üzerinde çalışıyor.\")"
   ],
   "id": "53c96c0b0e9bbd77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanılan GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T20:32:52.245814Z",
     "start_time": "2025-09-16T20:19:13.161675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE) if weights is not None else None)\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "def train_model(model,train_loader,val_loader,criterion,optimizer,epochs,device,model_name):\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_corrects = 0\n",
    "\n",
    "        for images,labels in tqdm(train_loader,desc=f\"Epoch {epoch+1}/{epochs} (Train)\"):\n",
    "            images,labels = images.to(DEVICE),labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _,predicts = torch.max(outputs,1)\n",
    "            train_corrects += torch.sum(predicts == labels.data)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} (Validation)\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(predictions == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc.item(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc.item()\n",
    "        })\n",
    "\n",
    "        # Save\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Model path\n",
    "            model_path = Path(f\"../../models/{model_name}_best_model.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model was saved: {model_name}\")\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "\n",
    "    # Save as csv\n",
    "    history_path = Path(f\"../../outputs/logs/{model_name}_training_history.csv\")\n",
    "    history_df.to_csv(history_path,index=False)\n",
    "    print(f\"Training metrics was saved: {model_name}_training_history.csv\")\n",
    "\n",
    "# Start training\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    epochs = EPOCHS,\n",
    "    device = DEVICE,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(\"\\n Finished\")\n"
   ],
   "id": "b82483fd19c4343e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/3 (Train):   0%|          | 0/1609 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7f730b2e7d148cba604a1bf718bf309"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1/3 (Validation):   0%|          | 0/345 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "640dc570dfcc4864b6b27ace1c5ffecb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 -> Train Loss: 2.2539, Train Acc: 0.4346, Val Loss: 1.4095, Val Acc: 0.6302\n",
      "Model was saved: mobilenet_v3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/3 (Train):   0%|          | 0/1609 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e8f7f0c6c2447cd9e964b9a80c739ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/3 (Validation):   0%|          | 0/345 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d781e1ac33b4c7691c63c0798be4b94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 -> Train Loss: 1.5366, Train Acc: 0.5770, Val Loss: 1.2304, Val Acc: 0.6749\n",
      "Model was saved: mobilenet_v3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3/3 (Train):   0%|          | 0/1609 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dedeaab962b496dbcf45dc7c1c13144"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3/3 (Validation):   0%|          | 0/345 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "354d2420f8644a488493a00a5ab30880"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 -> Train Loss: 1.3655, Train Acc: 0.6180, Val Loss: 1.1259, Val Acc: 0.7013\n",
      "Model was saved: mobilenet_v3\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\..\\outputs\\logs'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 80\u001B[39m\n\u001B[32m     77\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTraining metrics was saved: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_training_history.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     79\u001B[39m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     83\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     84\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     85\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     86\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     87\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     88\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_name\u001B[49m\n\u001B[32m     89\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     91\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m Finished\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 76\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, criterion, optimizer, epochs, device, model_name)\u001B[39m\n\u001B[32m     74\u001B[39m \u001B[38;5;66;03m# Save as csv\u001B[39;00m\n\u001B[32m     75\u001B[39m history_path = Path(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m../../outputs/logs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_training_history.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m \u001B[43mhistory_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhistory_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     77\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTraining metrics was saved: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_training_history.csv\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\diet_assistant_env\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[39m, in \u001B[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) > num_allow_args:\n\u001B[32m    328\u001B[39m     warnings.warn(\n\u001B[32m    329\u001B[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001B[32m    330\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m    331\u001B[39m         stacklevel=find_stack_level(),\n\u001B[32m    332\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\diet_assistant_env\\Lib\\site-packages\\pandas\\core\\generic.py:3986\u001B[39m, in \u001B[36mNDFrame.to_csv\u001B[39m\u001B[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[39m\n\u001B[32m   3975\u001B[39m df = \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.to_frame()\n\u001B[32m   3977\u001B[39m formatter = DataFrameFormatter(\n\u001B[32m   3978\u001B[39m     frame=df,\n\u001B[32m   3979\u001B[39m     header=header,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3983\u001B[39m     decimal=decimal,\n\u001B[32m   3984\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3986\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3987\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3988\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3989\u001B[39m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[43m=\u001B[49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3990\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3991\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3992\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3993\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3994\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3995\u001B[39m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3996\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3997\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3999\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4000\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4001\u001B[39m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4002\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4003\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\diet_assistant_env\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001B[39m, in \u001B[36mDataFrameRenderer.to_csv\u001B[39m\u001B[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[39m\n\u001B[32m    993\u001B[39m     created_buffer = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    995\u001B[39m csv_formatter = CSVFormatter(\n\u001B[32m    996\u001B[39m     path_or_buf=path_or_buf,\n\u001B[32m    997\u001B[39m     lineterminator=lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1012\u001B[39m     formatter=\u001B[38;5;28mself\u001B[39m.fmt,\n\u001B[32m   1013\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m \u001B[43mcsv_formatter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[32m   1017\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\diet_assistant_env\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001B[39m, in \u001B[36mCSVFormatter.save\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03mCreate the writer & save.\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    250\u001B[39m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[32m    259\u001B[39m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[32m    260\u001B[39m     \u001B[38;5;28mself\u001B[39m.writer = csvlib.writer(\n\u001B[32m    261\u001B[39m         handles.handle,\n\u001B[32m    262\u001B[39m         lineterminator=\u001B[38;5;28mself\u001B[39m.lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m    267\u001B[39m         quotechar=\u001B[38;5;28mself\u001B[39m.quotechar,\n\u001B[32m    268\u001B[39m     )\n\u001B[32m    270\u001B[39m     \u001B[38;5;28mself\u001B[39m._save()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\diet_assistant_env\\Lib\\site-packages\\pandas\\io\\common.py:749\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    747\u001B[39m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[32m    748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[32m--> \u001B[39m\u001B[32m749\u001B[39m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    751\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[32m    752\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m compression != \u001B[33m\"\u001B[39m\u001B[33mzstd\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    753\u001B[39m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\diet_assistant_env\\Lib\\site-packages\\pandas\\io\\common.py:616\u001B[39m, in \u001B[36mcheck_parent_directory\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    614\u001B[39m parent = Path(path).parent\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent.is_dir():\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[33mrf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCannot save file into a non-existent directory: \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mOSError\u001B[39m: Cannot save file into a non-existent directory: '..\\..\\outputs\\logs'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4ea495639f58534d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
